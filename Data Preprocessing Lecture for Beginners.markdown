# محاضرة: أساسيات معالجة البيانات لتحليل البيانات (Data Analysis) - الجزء الثاني

## **مقدمة**
مرحبًا يا شباب! النهاردة هنتكلم عن موضوع مهم جدًا في تحليل البيانات، وهو **معالجة البيانات (Data Preprocessing)**، وبالأخص جزئين رئيسيين: **تقليل البيانات (Data Reduction)** و**تحويل البيانات (Data Transformation)**. ليه الموضوع ده مهم؟ لأن البيانات اللي بنشتغل عليها في العالم الحقيقي بتبقى زي الغابة: كبيرة، معقدة، ومليانة "ضوضاء" (Noise). لو عايزين نستخلص معلومات مفيدة، لازم ننظفها وننظمها ونخليها أصغر وأسهل في التعامل. جاهزين؟ يلا بينا!

---

## **الجزء الأول: تقليل البيانات (Data Reduction)**

### **إيه هو Data Reduction؟**
تخيلوا إن عندكم مستودع مليان بضاعة (بيانات)، بس مش كل البضاعة دي لازمة. الهدف من **Data Reduction** إننا نقلل حجم البيانات (سواء عدد الأعمدة أو الصفوف) بدون ما نفقد المعلومات المهمة. ليه بنعمل كده؟ عشان:
1. **نقلل التخزين (Storage)**: بدل ما نخزن تيرا بايت من البيانات، نقدر نستخدم ميجا بايت.
2. **نسرّع التحليل**: معالجة بيانات أقل = وقت أقل.
3. **نحسن الدقة**: إزالة البيانات الغير ضرورية بتقلل التشويش وتخلّي النماذج (Models) أدق.

### **أنواع Data Reduction**
في نوعين رئيسيين لتقليل البيانات: **تقليل الأبعاد (Dimensionality Reduction)** و**تقليل عدد النقاط (Numerosity Reduction)**. خلونا نشرح كل نوع بمثال بسيط.

#### **1. Dimensionality Reduction (تقليل عدد الميزات)**
الميزات (Features) هي الأعمدة في جدول البيانات (زي السعر، المساحة، العمر). لو عندك 100 عمود، مش كلهم مهمين! هنا بنحاول نقلل عدد الأعمدة دي بطرق زكية.

##### **أ. Principal Component Analysis (PCA)**
- **إيه هو PCA؟** 
  PCA هو أسلوب ذكي بياخد الميزات (Features) اللي عندك وبيجمّعها في ميزات أقل، لكن بيحافظ على أهم المعلومات. بمعنى تاني، بدل ما تشتغل على 5 ميزات، ممكن تشتغل على ميزتين بس بيحتووا على أغلب المعلومات.
  
- **ازاي بيشتغل؟**
  1. **Normalization**: نخلّي كل البيانات في نفس النطاق (مثلًا من 0 لـ 1).
  2. **حساب التباين (Variance)**: نشوف إزاي البيانات بتتوزع وإيه الميزات اللي بتأثر أكتر.
  3. **إيجاد المتجهات الرئيسية (Principal Components)**: دي متجهات (خطوط) بتمثل أكبر قدر من التباين في البيانات.
  4. **إسقاط البيانات**: ننقل البيانات على الخطوط دي، فتتحول من أبعاد كتير (مثلًا 5 ميزات) لأبعاد أقل (مثلًا ميزتين).

- **مثال عملي** (مستوحى من [Towards Data Science](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)):
  تخيلوا إنكم مديرين تسويق في متجر إلكتروني. عندكم بيانات عن العملاء:
  - **X1**: المبلغ اللي بيصرفوه على الأجهزة الإلكترونية سنويًا (بالدولار).
  - **X2**: عدد قطع الملابس اللي اشتروها سنويًا.
  
  جدول البيانات:
  | العميل | X1 (إلكترونيات) | X2 (ملابس) |
  |--------|-------------------|-------------|
  | A      | 2000              | 10          |
  | B      | 4000              | 8           |
  | C      | 6000              | 6           |
  | D      | 8000              | 4           |

  لو طبقنا PCA:
  1. **Normalization**: نحول البيانات لنطاق موحد (مثلًا باستخدام Z-Score).
  2. نحسب **Covariance Matrix** ونجد الـ Eigenvectors (افترض إن أهم متجه هو [0.71, -0.71]).
  3. نضرب كل نقطة في المتجه ده لنحصل على قيمة جديدة (Customer Profile Score).
  
  النتيجة: بدل ما نشتغل على ميزتين (X1 وX2)، بنشتغل على ميزة واحدة تمثل العلاقة بينهم.

- **لماذا PCA مفيد؟**
  - بيقلل عدد الميزات، فالتحليل بيبقى أسرع.
  - بيحافظ على أغلب المعلومات المهمة.
  - **ملحوظة**: PCA بيشتغل بس على البيانات الرقمية (Numeric Data).

##### **ب. Feature Selection**
- **إيه هي؟**
  إزالة الميزات اللي مش مهمة، زي:
  - **Redundant Attributes**: ميزات مكررة (مثل سعر المنتج والضريبة، لأن الضريبة بتتحسب من السعر).
  - **Irrelevant Attributes**: ميزات مالهاش علاقة بالتحليل (مثل رقم تعريف الطالب لو بنحلل درجاته).

- **طرق الاختيار (Heuristic Search)**:
  1. **Best Single Attribute**: نختار أفضل ميزة واحدة بناءً على اختبارات الأهمية (مثل تأثيرها على التارجت).
  2. **Step-wise Forward Selection**: نبدأ بميزة واحدة، وبعدين نضيف أفضل ميزة تانية، وهكذا.
  3. **Step-wise Backward Elimination**: نبدأ بكل الميزات، ونشيل الأقل أهمية واحدة واحدة.

- **مثال عملي** (مستوحى من [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/)):
  لو عندك بيانات لتوقع سعر منزل بناءً على:
  - عدد الغرف.
  - المساحة.
  - عمر المبنى.
  - رقم تعريف العقار.
  
  هنا نقدر نشيل "رقم تعريف العقار" لأنه مش هيأثر على السعر (Irrelevant). ولو المساحة وعدد الغرف بيأثروا بنفس الطريقة، ممكن نشيل واحد منهم (Redundant).

##### **ج. Feature Creation**
- **إيه هي؟**
  إنشاء ميزات جديدة تجمع المعلومات بشكل أفضل.
- **مثال**:
  لو عندك بيانات عن تقييمات المستخدمين لمنتجات مختلفة، بدل ما تحلل كل تقييم لوحده، تقدر تعمل ميزة جديدة زي "متوسط التقييم لكل مستخدم". ده بيقلل عدد الميزات ويخلّي التحليل أسهل.

#### **2. Numerosity Reduction (تقليل عدد النقاط)**
هنا بنقلل عدد الصفوف (Data Points) في الجدول.

##### **أ. Parametric Methods**
- **Regression**:
  بدل ما نخزن كل البيانات، بنعمل نموذج (معادلة) بيوصف العلاقة بين المتغيرات.
  - **مثال** (مستوحى من [Khan Academy](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/introduction-to-regression/a/introduction-to-regression)):
    لو عندك بيانات عن أسعار المنازل بناءً على مساحتها:
    | المساحة (X) | السعر (Y) |
    |--------------|------------|
    | 100          | 200,000    |
    | 200          | 400,000    |
    | 300          | 600,000    |
    
    نقدر نستخدم معادلة خطية: \( Y = 2000X \).
    بدل ما نخزن كل النقاط، بنخزن بس المعامل (2000). لو عايزين نعرف سعر بيت مساحته 150، بنستخدم المعادلة: \( Y = 2000 \times 150 = 300,000 \).

- **Log-Linear Models**:
  بيستخدموا لتحليل البيانات النوعية (Categorical)، زي احتمال شراء عميل لمنتج معين بناءً على إجابات "نعم/لا".

##### **ب. Non-Parametric Methods**
- **Histograms**:
  - **Equal-width**: نقسم البيانات لفترات متساوية الحجم.
    - **مثال**: أعمار العملاء من 10 لـ 70، لو عايزين 3 فترات: [10-30], [31-50], [51-70].
  - **Equal-frequency**: نقسم بحيث كل فترة فيها نفس العدد من النقاط.
    - **مثال**: لو عندك 100 عميل، كل فترة فيها 33 عميل تقريبًا.
- **Clustering**:
  نجمع البيانات المتشابهة في مجموعات (Clusters) ونخزن بس تمثيل المجموعة (مثل المتوسط).
  - **مثال** (مستوحى من [Simplilearn](https://www.simplilearn.com/tutorials/data-science-tutorial/clustering-in-data-science)):
    لو عندك بيانات عن مواقع العملاء على الخريطة، نقدر نجمعهم في 3 مجموعات (شمال، وسط، جنوب) ونخزن بس مركز كل مجموعة.
- **Sampling**:
  نأخذ عينة صغيرة تمثل البيانات الكبيرة.
  - **Simple Random Sampling**: نختار بشكل عشوائي.
  - **Stratified Sampling**: نقسم البيانات لمجموعات (مثل رجال/نساء) ونأخذ عينة من كل مجموعة.
  - **مثال**: لو عندك مليون عميل، نأخذ عينة 10,000 عميل يمثلوا كل الفئات العمرية.

##### **ج. Data Cube Aggregation**
- بناءً على مستويات التجميع في قاعدة البيانات.
- **مثال**:
  لو عندك بيانات مبيعات مفصلة (منتج، منطقة، تاريخ)، نقدر نجمّع المبيعات حسب المنطقة فقط (بدل كل منتج).

##### **د. Data Compression**
- تقليل حجم البيانات باستخدام ضغط.
- **أنواع**:
  - **Lossless**: (للنصوص) بيحافظ على كل البيانات.
  - **Lossy**: (للصوت/الفيديو) بيضيع جزء بسيط من البيانات لكن بيقلل الحجم جدًا.
- **مثال** (مستوحى من [GeeksforGeeks](https://www.geeksforgeeks.org/introduction-to-data-compression/)):
  لو عندك ملف صوتي 100 ميجا، الضغط بيخليه 10 ميجا، لكن لما تفك الضغط، ممكن تلاقي جودة الصوت نقصت شوية.

---

## **الجزء الثاني: تحويل البيانات (Data Transformation)**

### **إيه هو Data Transformation؟**
تحويل البيانات يعني تغيير شكلها عشان تبقى جاهزة للتحليل. زي لما تغيّر مقاسات ملابس من سنتيمتر لإنش، أو تحول أرقام لكلمات (مثل عمر 20 يتحول لـ "شاب").

### **أنواع Data Transformation**

#### **1. Normalization**
- **إيه هي؟**
  تحويل البيانات لنطاق موحد عشان نقدر نقارن بينها بسهولة.
- **الطرق**:
  1. **Min-Max Normalization**:
     - تحول البيانات لنطاق معين (مثل [0,1]).
     - **المعادلة**:
       \[
       v' = \frac{v - \min_A}{\max_A - \min_A} (\text{new_max}_A - \text{new_min}_A) + \text{new_min}_A
       \]
     - **مثال**:
       لو عندك رواتب من 12,000 لـ 98,000، وتريد تحول راتب 73,600 لنطاق [0,1]:
       \[
       v' = \frac{73,600 - 12,000}{98,000 - 12,000} (1 - 0) + 0 = 0.716
       \]
  2. **Z-Score Normalization**:
     - تحول البيانات بناءً على المتوسط والانحراف المعياري.
     - **المعادلة**:
       \[
       v' = \frac{v - \mu_A}{\sigma_A}
       \]
     - **مثال**:
       لو المتوسط 54,000 والانحراف 16,000، راتب 73,600 يتحول لـ:
       \[
       v' = \frac{73,600 - 54,000}{16,000} = 1.225
       \]
  3. **Decimal Scaling**:
     - تقسم القيم على قوى 10.
     - **مثال**:
       لو عندك قيمة 1500، نقسمها على 1000، تصبح 1.5.

#### **2. Discretization**
- **إيه هي؟**
  تحويل البيانات المستمرة (مثل الأرقام) لفئات (مثل "شاب، بالغ، كبير").
- **الطرق**:
  - **Binning**:
    - **Equal-width**: تقسيم النطاق لفترات متساوية.
    - **Equal-frequency**: تقسيم بحيث كل فترة فيها نفس العدد.
    - **مثال**:
      أسعار: [4, 8, 15, 21, 25, 28]. لو نقسمها لـ 3 فترات متساوية التردد:
      - Bin 1: [4, 8, 15]
      - Bin 2: [21, 25]
      - Bin 3: [28]
      ثم نعمل Smoothing (مثلًا بالمتوسط):
      - Bin 1: [9, 9, 9]
      - Bin 2: [23, 23]
      - Bin 3: [28]
  - **Clustering**: تجميع القيم المتشابهة.
  - **Decision Tree**: تقسيم بناءً على التارجت (Supervised).

#### **3. Concept Hierarchy Generation**
- **إيه هي؟**
  تنظيم البيانات في مستويات هرمية (مثل: شارع < مدينة < محافظة < بلد).
- **مثال** (مستوحى من [Data Warehouse Concepts](https://www.geeksforgeeks.org/data-warehouse-concepts/)):
  لو عندك بيانات مبيعات:
  - **المستوى المنخفض**: مبيعات كل منتج في كل شارع.
  - **المستوى العالي**: مبيعات كل المنتجات في محافظة.
  تقدر تحول من المستوى المنخفض (Drilling Down) للعالي (Rolling Up).

- **طرق الإنشاء**:
  - **يدوي**: خبراء يحددوا التسلسل (مثل: القاهرة > مدينة نصر > شارع الطيران).
  - **تلقائي**: بناءً على عدد القيم المميزة (مثل: شارع: 674,339 قيمة > مدينة: 3,567 > بلد: 15).

---

## **أسئلة وتدريبات للطلبة**
1. **سؤال بسيط**: لو عندك عمود "سعر المنتج" وعمود "الضريبة"، ليه ممكن نشيل عمود الضريبة؟ (تلميح: فكر في Redundant Attributes).
2. **تدريب عملي**:
   - خد البيانات دي: [10, 20, 30, 40, 50]. طبق Equal-width Binning لتقسيمها لـ 2 فترة، ثم اعمل Smoothing بالمتوسط.
3. **سؤال تفكير**: لو عندك مليون عميل، إزاي تختار عينة 10,000 عميل بشكل يمثل البيانات كويس؟ (تلميح: فكر في Stratified Sampling).
4. **سؤال تحليلي**: لو عايز تحلل مبيعات متجر، إزاي Concept Hierarchy بتساعدك توفر وقت وتخزين؟

---

## **نصائح للشباب المبتدئين**
1. **ابدأ بفهم البيانات**: اسأل نفسك دايمًا: إيه البيانات المهمة؟ وإيه اللي مش ضروري؟
2. **جرب أدوات بسيطة**: استخدم Python (مكتبة `scikit-learn`) لتجرب PCA أو Normalization. في دروس مجانية على [Coursera](https://www.coursera.org/learn/data-science-python) بتشرح الكلام ده.
3. **ركز على الأمثلة العملية**: لو بتحلل بيانات متجر، فكر إزاي تقلل عدد الأعمدة (مثل PCA) أو عدد العملاء (مثل Sampling).
4. **ما تخافش من الأخطاء**: تحليل البيانات زي الطبخ، لازم تجرب وتعيد لحد ما توصل للطعم الصح!

---

## **مراجع إضافية**
- [Towards Data Science](https://towardsdatascience.com): شرح PCA وFeature Selection بأمثلة عملية.
- [GeeksforGeeks](https://www.geeksforgeeks.org): مقالات عن Data Compression وData Warehouse.
- [Analytics Vidhya](https://www.analyticsvidhya.com): دروس عن Feature Selection وClustering.
- كتاب *Data Mining: Concepts and Techniques* (Jiawei Han وآخرون).